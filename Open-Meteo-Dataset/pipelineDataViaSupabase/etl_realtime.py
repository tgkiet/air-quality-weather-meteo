# T·ª± ƒë·ªông c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi nh·∫•t t·ª´ Open-Meteo API v√† l∆∞u tr·ªØ v√†o Supabase
# L·ªãch ch·∫°y l√† m·ªói gi·ªù

# --- 1. Import th∆∞ vi·ªán ---
import logging
import random
import uuid
import pandas as pd
import os
from datetime import datetime, timezone
import requests
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
import openmeteo_requests
from retry_requests import retry
import time

# --- Logging setup ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_FILE_PATH = os.path.join(BASE_DIR, "etl_realtime.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE_PATH, encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("etl_realtime")


# --- H·∫±ng s·ªë to√†n c·ª•c ---
METADATA_FILE_PATH = os.path.join(BASE_DIR, "../stations_metadata.csv") # ƒê∆∞·ªùng d·∫´n an to√†n h∆°n
DB_TABLE_NAME = "air_quality_forecast_data"


# -- 3. ƒê·ªãnh nghƒ©a c√°c h√†m ch·ª©c nƒÉng ---

def get_db_engine():
    """
    H√†m n√†y ƒë·ªçc chu·ªói k·∫øt n·ªëi t·ª´ .env v√† t·∫°o m·ªôt SQLAlchemy engine
    Nhi·ªám v·ª• duy nh·∫•t c·ªßa function n√†y l√† t·∫°o  k·∫øt n·ªëi.
    """
    load_dotenv()
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise ValueError("L·ªói: Kh√¥ng t√¨m th·∫•y DATABASE_URL trong file .env")
    logger.info(" K·∫øt n·ªëi database ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng.")
    # pool_pre_ping gi√∫p ph√°t hi·ªán connection dead v√† reconnect t·ª± ƒë·ªông
    return create_engine(db_url, pool_pre_ping=True)

def retry_execute(conn, query, retries=3, delay_base=1.0):
    """
    Th·ª±c thi truy v·∫•n sql v·ªõi c∆° ch·∫ø retry n·∫øu g·∫∑p deadlock
    """
    last_exception = None
    for attempt in range(retries):
        try:
            conn.execute(text(query))
            return
        except Exception as e:
            last_exception = e
            msg = str(e).lower()
            if any(err in msg for err in ["deadlock detected", "could not obtain lock", "serialization failure"]):
                wait = delay_base * (2 ** attempt) + random.random()
                logger.warning(f"  Ph√°t hi·ªán Deadlock/lock - retry sau {wait:.1f}s (l·∫ßn {attempt + 1}/{retries})...")
                time.sleep(wait)
            else:
                logger.error(f"L·ªói SQL kh√¥ng th·ªÉ retry: {e}")
                raise
    raise RuntimeError(f"Qu√° s·ªë l·∫ßn retry do deadlock/lock. L·ªói cu·ªëi c√πng: {last_exception}")

def fetch_recent_data(stations_df):
    """
    G·ªçi API Open-Meteo ƒë·ªÉ l·∫•y d·ªØ li·ªáu 3 ng√†y g·∫ßn nh·∫•t.
    Th·ª±c hi·ªán hai l·ªánh g·ªçi API ri√™ng bi·ªát, c·∫£ hai ƒë·ªÅu d√πng `past_days`.
    """
    logger.info("B·∫Øt ƒë·∫ßu h√†m fetch_recent_data...")
    
    retry_session = retry(requests.Session(), retries=5, backoff_factor=0.2)
    openmeteo = openmeteo_requests.Client(session=retry_session)

    all_station_dfs = []
    num_past_days = 3

    for index, station in stations_df.iterrows():
        loc_id = station['location_id']
        lat = station['lat']
        lon = station['lon']
        
        logger.info(f"  -> ƒêang x·ª≠ l√Ω v·ªã tr√≠ tr·∫°m ID: {loc_id} cho {num_past_days} ng√†y qua...")
        
        df_weather = pd.DataFrame()
        df_aq = pd.DataFrame()

        try:
            # === 1. L·ªÜNH G·ªåI API TH·ªúI TI·∫æT (WEATHER FORECAST) ===
            weather_url = "https://api.open-meteo.com/v1/forecast"
            weather_params = {
                "latitude": lat, "longitude": lon,
                "hourly": [
                    "temperature_2m", "relative_humidity_2m", "precipitation", "rain", 
                    "wind_speed_10m", "wind_direction_10m", "pressure_msl", "boundary_layer_height"
                ],
                "past_days": num_past_days,
                "forecast_days": 1
            }
            weather_responses = openmeteo.weather_api(weather_url, params=weather_params)
            weather_response = weather_responses[0]

            hourly = weather_response.Hourly()
            # D√πng pd.date_range ƒë·ªÉ ƒë·∫£m b·∫£o chu·ªói th·ªùi gian lu√¥n ch√≠nh x√°c
            df_weather = pd.DataFrame(data={"datetime": pd.date_range(
                start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
                end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
                freq=pd.Timedelta(seconds=hourly.Interval()),
                inclusive="left"
            )})
            
            for i, var_name in enumerate(weather_params["hourly"]):
                values = hourly.Variables(i).ValuesAsNumpy()
                df_weather[var_name] = values[:len(df_weather)]
            logger.info("   - L·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt th√†nh c√¥ng.")

        except Exception as e:
            logger.warning(f"  - C·∫£nh b√°o: L·ªói khi l·∫•y d·ªØ li·ªáu TH·ªúI TI·∫æT cho tr·∫°m {loc_id}: {e}")
            # N·∫øu l·ªói, ch√∫ng ta v·∫´n ti·∫øp t·ª•c ƒë·ªÉ th·ª≠ l·∫•y d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠

        try:
            # === 2. L·ªÜNH G·ªåI API CH·∫§T L∆Ø·ª¢NG KH√îNG KH√ç (AIR QUALITY) ===
            aq_url = "https://air-quality-api.open-meteo.com/v1/air-quality"
            aq_params = {
                "latitude": lat, "longitude": lon,
                "hourly": ["pm10", "pm2_5", "carbon_monoxide", "nitrogen_dioxide", "sulphur_dioxide", "ozone"],
                "past_days": num_past_days,
                "forecast_days": 1
            }
            aq_responses = openmeteo.weather_api(aq_url, params=aq_params)
            aq_response = aq_responses[0]
            
            hourly = aq_response.Hourly()
            df_aq = pd.DataFrame(data={"datetime": pd.date_range(
                start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
                end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
                freq=pd.Timedelta(seconds=hourly.Interval()),
                inclusive="left"
            )})
            
            for i, var_name in enumerate(aq_params["hourly"]):
                values = hourly.Variables(i).ValuesAsNumpy()
                df_aq[f"{var_name}_cams"] = values[:len(df_aq)]
            logger.info("     - L·∫•y d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ th√†nh c√¥ng.")

        except Exception as e:
            logger.warning(f"     - C·∫£nh b√°o: L·ªói khi l·∫•y d·ªØ li·ªáu CH·∫§T L∆Ø·ª¢NG KH√îNG KH√ç cho tr·∫°m {loc_id}: {e}")
            # N·∫øu l·ªói, ch√∫ng ta v·∫´n c√≥ th·ªÉ c√≥ d·ªØ li·ªáu th·ªùi ti·∫øt
            
        # === 3. MERGE HAI DATAFRAME L·∫†I ===
        # Ch·ªâ g·ªôp n·∫øu c√≥ √≠t nh·∫•t m·ªôt trong hai DataFrame kh√¥ng r·ªóng
        if not df_weather.empty or not df_aq.empty:
            if not df_weather.empty and not df_aq.empty:
                df_station_combined = pd.merge(df_weather, df_aq, on='datetime', how='outer')
            elif not df_weather.empty:
                df_station_combined = df_weather
            else:
                df_station_combined = df_aq
                
            df_station_combined['location_id'] = loc_id
            df_station_combined['lat'] = lat
            df_station_combined['lon'] = lon
            
            all_station_dfs.append(df_station_combined)
            logger.info(f"    -> Th√†nh c√¥ng. ƒê√£ x·ª≠ l√Ω tr·∫°m {loc_id} ({len(df_station_combined)} d√≤ng).")

        else:
            logger.warning(f"    -> Th·∫•t b·∫°i: Kh√¥ng l·∫•y ƒë∆∞·ª£c c·∫£ hai lo·∫°i d·ªØ li·ªáu cho tr·∫°m {loc_id}.")


    if not all_station_dfs:
        logger.info("Kh√¥ng l·∫•y ƒë∆∞·ª£c b·∫•t k·ª≥ d·ªØ li·ªáu m·ªõi n√†o t·ª´ API.")
        return None

    final_df = pd.concat(all_station_dfs, ignore_index=True)
    
    final_df = final_df[final_df['datetime'] <= datetime.now(timezone.utc)].copy()
    
    logger.info(f"Ho√†n t·∫•t fetch_recent_data. T·ªïng c·ªông {len(final_df)} d√≤ng ƒë∆∞·ª£c l·∫•y v·ªÅ.")
    return final_df
        

def upsert_data(engine, df: pd.DataFrame, table_name: str, pipeline_id: str = None):
    """
    Ghi DataFrame v√†o PostgreSQL m·ªôt c√°ch nguy√™n t·ª≠ (atomic), an to√†n v√† hi·ªáu qu·∫£,
    s·ª≠ d·ª•ng m·ªôt transaction duy nh·∫•t. T∆∞∆°ng th√≠ch v·ªõi Supabase.
    """
    
    if df is None or df.empty:
        logger.warning(" Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ th·ª±c hi·ªán UpSert. B·ªè qua.")
        return
    
    # Chu·∫©n b·ªã t√™n b·∫£ng t·∫°m duy nh·∫•t
    # Bao b·ªçc b·∫±ng ngo·∫∑c k√©p ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n trong c√°c c√¢u l·ªánh SQL th√¥
    temp_table_name_quoted = f'"temp_{table_name}_{uuid.uuid4().hex[:8]}"'
    # pandas.to_sql c·∫ßn t√™n kh√¥ng c√≥ ngo·∫∑c k√©p
    temp_table_name_unquoted = temp_table_name_quoted.strip('"')

    batch_id = pipeline_id or uuid.uuid4().hex[:6]
    logger.info(f" [Pipeline {batch_id}] b·∫Øt ƒë·∫ßu upsert {len(df)} d√≤ng v√†o b·∫£ng '{table_name}' ...")
    
    # M·ªü k·∫øt n·ªëi m·ªôt l·∫ßn duy nh·∫•t cho to√†n b·ªô t√°c v·ª•
    with engine.connect() as conn:
        try:
            # --- B·∫ÆT ƒê·∫¶U M·ªòT TRANSACTION DUY NH·∫§T ---
            # To√†n b·ªô logic nghi·ªáp v·ª• s·∫Ω n·∫±m trong kh·ªëi n√†y.
            # N√≥ s·∫Ω t·ª± ƒë·ªông COMMIT khi k·∫øt th√∫c th√†nh c√¥ng, ho·∫∑c ROLLBACK n·∫øu c√≥ l·ªói.
            with conn.begin():
                
                # B∆∞·ªõc A: Ghi d·ªØ li·ªáu v√†o b·∫£ng t·∫°m
                logger.info(f"  A. Ghi d·ªØ li·ªáu v√†o b·∫£ng t·∫°m '{temp_table_name_unquoted}'...")
                df.to_sql(
                    temp_table_name_unquoted,
                    conn, # S·ª≠ d·ª•ng connection c·ªßa transaction hi·ªán t·∫°i
                    if_exists="replace",
                    index=False,
                    method='multi',
                    chunksize=5000
                )
                logger.info("     -> Ghi v√†o b·∫£ng t·∫°m th√†nh c√¥ng.")

                # B∆∞·ªõc B: Th·ª±c thi logic Upsert t·ª´ b·∫£ng t·∫°m
                logger.info("  B. Th·ª±c thi l·ªánh UPSERT...")
                
                # L·∫•y danh s√°ch c·ªôt t·ª´ DataFrame ƒë·ªÉ ƒë·∫£m b·∫£o kh·ªõp 100%
                cols_quoted = ", ".join([f'"{c.lower()}"' for c in df.columns])
                
                # Upsert v·ªõi RETURNING
                upsert_query = f"""
                INSERT INTO public."{table_name}" ({cols_quoted})
                SELECT {cols_quoted} FROM {temp_table_name_quoted}
                ON CONFLICT (location_id, datetime) DO NOTHING
                RETURNING 1;
                """
                result = conn.execute(text(upsert_query))
                rows_inserted = result.rowcount

                logger.info(f" -> L·ªánh Upsert ƒë√£ ƒë∆∞·ª£c th·ª±c thi th√†nh c√¥ng, th·ª±c s·ª± insert {rows_inserted} d√≤ng.")

                # L∆∞u √Ω: B·∫£ng t·∫°m (kh√¥ng ph·∫£i l√† TEMP TABLE) ƒë∆∞·ª£c t·∫°o trong transaction n√†y
                # s·∫Ω b·ªã rollback v√† bi·∫øn m·∫•t n·∫øu transaction th·∫•t b·∫°i.
                # N·∫øu th√†nh c√¥ng, n√≥ v·∫´n t·ªìn t·∫°i cho ƒë·∫øn khi b·ªã d·ªçn d·∫πp.
                
            # Transaction k·∫øt th√∫c, COMMIT ƒë√£ ƒë∆∞·ª£c g·ªçi t·ª± ƒë·ªông.
            logger.info("  ‚úÖ Giao d·ªãch Upsert ho√†n t·∫•t v√† ƒë√£ ƒë∆∞·ª£c COMMIT.")

        except Exception:
            # Log l·ªói v√† th√¥ng b√°o v·ªÅ vi·ªác rollback t·ª± ƒë·ªông
            logger.error("\n‚ùå L·ªói trong qu√° tr√¨nh Upsert. Transaction ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông ROLLBACK.", exc_info=True)

        finally:
            # --- B∆Ø·ªöC C: D·ªåN D·∫∏P ---
            # Kh·ªëi `finally` ƒë·∫£m b·∫£o vi·ªác d·ªçn d·∫πp lu√¥n ƒë∆∞·ª£c th·ª±c thi,
            # d√π transaction ·ªü tr√™n th√†nh c√¥ng hay th·∫•t b·∫°i.
            logger.info(f"  -> C. D·ªçn d·∫πp b·∫£ng t·∫°m {temp_table_name_quoted}...")
            try:
                # Th·ª±c thi l·ªánh DROP TABLE tr√™n c√πng m·ªôt connection
                # Kh√¥ng c·∫ßn transaction ri√™ng cho l·ªánh n√†y trong SQLAlchemy 2.x
                conn.execute(text(f'DROP TABLE IF EXISTS {temp_table_name_quoted};'))
                conn.commit() # C·∫ßn commit t∆∞·ªùng minh cho l·ªánh ch·∫°y ngo√†i `with conn.begin()`
                logger.info("     -> D·ªçn d·∫πp b·∫£ng t·∫°m th√†nh c√¥ng.")
            except Exception as cleanup_e:
                logger.warning(f"     -> C·∫£nh b√°o: L·ªói khi d·ªçn d·∫πp b·∫£ng t·∫°m: {cleanup_e}")
                            
        logger.info(f"üèÅ [Pipeline {batch_id}] Ho√†n t·∫•t upsert cho b·∫£ng '{table_name}'.\n")


# --- 4. H√†m ƒëi·ªÅu ph·ªëi ch√≠nh (Main orchestrator function) --- 
def run_realtime_etl():
    """
    H√†m ch√≠nh ƒë·ªÉ ƒëi·ªÅu ph·ªëi qu√° tr√¨nh ETL.
    """
    logger.info("==================================================")
    logger.info(f"B·∫ÆT ƒê·∫¶U ETL PIPELINE L√öC: {datetime.now()}")
    logger.info("==================================================")
    start_time = time.time()
    
    try: 
        # B∆∞·ªõc A: ƒê·ªçc metadata
        logger.info(f"\n [B∆∞·ªõc 1/3] ƒêang ƒë·ªçc metadata t·ª´ '{METADATA_FILE_PATH}'...")
        if not os.path.exists(METADATA_FILE_PATH):
            raise FileNotFoundError(f"L·ªói: Kh√¥ng t√¨m th·∫•y file metadata '{METADATA_FILE_PATH}'.")
        df_metadata = pd.read_csv(METADATA_FILE_PATH)
        logger.info(f" -> ƒê·ªçc th√†nh c√¥ng th√¥ng tin c·ªßa {len(df_metadata)} tr·∫°m.")
        
        # B∆∞·ªõc B: L·∫•y d·ªØ li·ªáu m·ªõi (Extract & Transform)
        logger.info("\n [B∆∞·ªõc 2/3] ƒêang l·∫•y d·ªØ li·ªáu g·∫ßn ƒë√¢y t·ª´ Open-Meteo...")
        recent_data_df = fetch_recent_data(df_metadata)
        
        # B∆∞·ªõc C: T·∫£i d·ªØ li·ªáu v√†o DB (Load)
        logger.info("\n [B∆∞·ªõc 3/3] ƒêang t·∫£i d·ªØ li·ªáu l√™n database...")
        if recent_data_df is not None and not recent_data_df.empty:
            db_engine = get_db_engine()
            upsert_data(db_engine, recent_data_df, DB_TABLE_NAME)
        else:
            logger.info(" -> Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ t·∫£i l√™n.")
    
    except Exception as e:
        logger.exception("ETL JOB TH·∫§T B·∫†I !!!")
        logger.warning(f"L·ªói: {e}")
    
    finally:
        end_time = time.time()
        logger.info("\n==================================================")
        logger.info(f"K·∫æT TH√öC ETL JOB. T·ªîNG TH·ªúI GIAN: {end_time - start_time:.2f} GI√ÇY.")
        logger.info("==================================================")
    
#--- 5. ƒêi·ªÉm b·∫Øt ƒë·∫ßu th·ª±c thi c·ªßa script ---
if __name__ == "__main__":
    run_realtime_etl()
# T·ª± ƒë·ªông c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi nh·∫•t t·ª´ Open-Meteo API v√† l∆∞u tr·ªØ v√†o Supabase
# L·ªãch ch·∫°y l√† m·ªói gi·ªù

# --- 1. Import th∆∞ vi·ªán ---
import logging
import random
import uuid
import pandas as pd
import os
from datetime import datetime, timezone
import requests
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
import openmeteo_requests
from retry_requests import retry
import time

# --- 1b. Logging setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("etl_realtime")

# --- 2. ƒê·ªãnh nghƒ©a c√°c h·∫±ng s·ªë to√†n c·ª•c ---
METADATA_FILE_PATH = "../stations_metadata.csv"
DB_TABLE_NAME = "air_quality_forecast_data"


# -- 3. ƒê·ªãnh nghƒ©a c√°c h√†m ch·ª©c nƒÉng ---

def get_db_engine():
    """
    H√†m n√†y ƒë·ªçc chu·ªói k·∫øt n·ªëi t·ª´ .env v√† t·∫°o m·ªôt SQLAlchemy engine
    Nhi·ªám v·ª• duy nh·∫•t c·ªßa function n√†y l√† t·∫°o  k·∫øt n·ªëi.
    """
    load_dotenv()
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise ValueError("L·ªói: Kh√¥ng t√¨m th·∫•y DATABASE_URL trong file .env")
    logger.info(" K·∫øt n·ªëi database ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng.")
    # pool_pre_ping gi√∫p ph√°t hi·ªán connection dead v√† reconnect t·ª± ƒë·ªông
    return create_engine(db_url, pool_pre_ping=True)

def retry_execute(conn, query, retries=3, delay_base=1.0):
    """
    Th·ª±c thi truy v·∫•n sql v·ªõi c∆° ch·∫ø retry n·∫øu g·∫∑p deadlock
    """
    for attempt in range(retries):
        try:
            conn.execute(text(query))
            return
        except Exception as e:
            msg = str(e).lower()
            if "deadlock detected" in msg or "could not obtain lock" in msg or "serialization failure" in msg:
                wait = delay_base * (2 ** attempt) + random.random()
                print(f"  Ph√°t hi·ªán Deadlock/lock - retry sau {wait:.1f}s (l·∫ßn {attempt + 1}/{retries})...")
                time.sleep(wait)
            else:
                raise
    raise RuntimeError(f"Qu√° s·ªë l·∫ßn retry do deadlock/lock. L·ªói cu·ªëi c√πng: {e}")

def fetch_recent_data(stations_df):
    """
    G·ªçi API Open-Meteo ƒë·ªÉ l·∫•y d·ªØ li·ªáu 3 ng√†y g·∫ßn nh·∫•t.
    Th·ª±c hi·ªán hai l·ªánh g·ªçi API ri√™ng bi·ªát, c·∫£ hai ƒë·ªÅu d√πng `past_days`.
    """
    logger.info("LOG: B·∫Øt ƒë·∫ßu h√†m fetch_recent_data...")
    
    retry_session = retry(requests.Session(), retries=5, backoff_factor=0.2)
    openmeteo = openmeteo_requests.Client(session=retry_session)

    all_station_dfs = []
    num_past_days = 3

    for index, station in stations_df.iterrows():
        loc_id = station['location_id']
        lat = station['lat']
        lon = station['lon']
        
        logger.info(f"  -> ƒêang x·ª≠ l√Ω v·ªã tr√≠ tr·∫°m ID: {loc_id} cho {num_past_days} ng√†y qua...")
        
        df_weather = pd.DataFrame()
        df_aq = pd.DataFrame()

        try:
            # === 1. L·ªÜNH G·ªåI API TH·ªúI TI·∫æT (WEATHER FORECAST) ===
            weather_url = "https://api.open-meteo.com/v1/forecast"
            weather_params = {
                "latitude": lat, "longitude": lon,
                "hourly": [
                    "temperature_2m", "relative_humidity_2m", "precipitation", "rain", 
                    "wind_speed_10m", "wind_direction_10m", "pressure_msl", "boundary_layer_height"
                ],
                "past_days": num_past_days,
                "forecast_days": 1
            }
            weather_responses = openmeteo.weather_api(weather_url, params=weather_params)
            weather_response = weather_responses[0]

            hourly = weather_response.Hourly()
            # D√πng pd.date_range ƒë·ªÉ ƒë·∫£m b·∫£o chu·ªói th·ªùi gian lu√¥n ch√≠nh x√°c
            df_weather = pd.DataFrame(data={"datetime": pd.date_range(
                start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
                end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
                freq=pd.Timedelta(seconds=hourly.Interval()),
                inclusive="left"
            )})
            for i, var_name in enumerate(weather_params["hourly"]):
                values = hourly.Variables(i).ValuesAsNumpy()
                df_weather[var_name] = values[:len(df_weather)]
            logger.info("   - L·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt th√†nh c√¥ng.")

        except Exception as e:
            logger.warning(f"  - C·∫£nh b√°o: L·ªói khi l·∫•y d·ªØ li·ªáu TH·ªúI TI·∫æT cho tr·∫°m {loc_id}: {e}")
            # N·∫øu l·ªói, ch√∫ng ta v·∫´n ti·∫øp t·ª•c ƒë·ªÉ th·ª≠ l·∫•y d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠

        try:
            # === 2. L·ªÜNH G·ªåI API CH·∫§T L∆Ø·ª¢NG KH√îNG KH√ç (AIR QUALITY) ===
            aq_url = "https://air-quality-api.open-meteo.com/v1/air-quality"
            aq_params = {
                "latitude": lat, "longitude": lon,
                "hourly": ["pm10", "pm2_5", "carbon_monoxide", "nitrogen_dioxide", "sulphur_dioxide", "ozone"],
                "past_days": num_past_days,
                "forecast_days": 1
            }
            aq_responses = openmeteo.weather_api(aq_url, params=aq_params)
            aq_response = aq_responses[0]
            
            hourly = aq_response.Hourly()
            df_aq = pd.DataFrame(data={"datetime": pd.date_range(
                start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
                end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
                freq=pd.Timedelta(seconds=hourly.Interval()),
                inclusive="left"
            )})
            for i, var_name in enumerate(aq_params["hourly"]):
                values = hourly.Variables(i).ValuesAsNumpy()
                df_aq[f"{var_name}_cams"] = values[:len(df_aq)]
            logger.info("     - L·∫•y d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ th√†nh c√¥ng.")

        except Exception as e:
            logger.warning(f"     - C·∫£nh b√°o: L·ªói khi l·∫•y d·ªØ li·ªáu CH·∫§T L∆Ø·ª¢NG KH√îNG KH√ç cho tr·∫°m {loc_id}: {e}")
            # N·∫øu l·ªói, ch√∫ng ta v·∫´n c√≥ th·ªÉ c√≥ d·ªØ li·ªáu th·ªùi ti·∫øt
            
        # === 3. G·ªòP (MERGE) HAI DATAFRAME L·∫†I ===
        # Ch·ªâ g·ªôp n·∫øu c√≥ √≠t nh·∫•t m·ªôt trong hai DataFrame kh√¥ng r·ªóng
        if not df_weather.empty or not df_aq.empty:
            if not df_weather.empty and not df_aq.empty:
                df_station_combined = pd.merge(df_weather, df_aq, on='datetime', how='outer')
            elif not df_weather.empty:
                df_station_combined = df_weather
            else:
                df_station_combined = df_aq
                
            df_station_combined['location_id'] = loc_id
            all_station_dfs.append(df_station_combined)
            logger.info(f"    -> Th√†nh c√¥ng. ƒê√£ x·ª≠ l√Ω tr·∫°m {loc_id}.")
        else:
            logger.warning(f"    -> Th·∫•t b·∫°i: Kh√¥ng l·∫•y ƒë∆∞·ª£c c·∫£ hai lo·∫°i d·ªØ li·ªáu cho tr·∫°m {loc_id}.")


    if not all_station_dfs:
        logger.info("LOG: Kh√¥ng l·∫•y ƒë∆∞·ª£c b·∫•t k·ª≥ d·ªØ li·ªáu m·ªõi n√†o t·ª´ API.")
        return None

    final_df = pd.concat(all_station_dfs, ignore_index=True)
    
    final_df = final_df[final_df['datetime'] <= datetime.now(timezone.utc)].copy()
    
    logger.info(f"LOG: Ho√†n t·∫•t fetch_recent_data. T·ªïng c·ªông {len(final_df)} d√≤ng ƒë∆∞·ª£c l·∫•y v·ªÅ.")
    return final_df
        

def upsert_data(engine, df: pd.DataFrame, table_name: str, pipeline_id: str = None):
    """
    Ghi DataFrame v√†o PostgreSQL an to√†n v√† hi·ªáu qu·∫£,
    d√πng UNLOGGED TABLE t·∫°m, transaction ng·∫Øn v√† c∆° ch·∫ø retry ch·ªëng deadlock.
    
    Args:
        engine: SQLAlchemy engine.
        df: pandas DataFrame c·∫ßn upsert.
        table_name: t√™n b·∫£ng ƒë√≠ch trong database.
        pipeline_id: m√£ ƒë·ªãnh danh pipeline (t√πy ch·ªçn, ch·ªâ ƒë·ªÉ log).
    """
    
    if df is None or df.empty:
        logger.warning(" Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ th·ª±c hi·ªán UpSert. B·ªè qua.")
        return
    
    df.columns = [col.lower() for col in df.columns]
    temp_table_name = f"temp_{table_name}_{uuid.uuid4().hex[:8]}"
    batch_id = pipeline_id or uuid.uuid4().hex[:6]
    logger.info(f" [Pipeline {batch_id}] b·∫Øt ƒë·∫ßu upsert {len(df)} d√≤ng v√†o b·∫£ng '{table_name}' ...")
    
    try:
        # B∆∞·ªõc A: Ghi d·ªØ li·ªáu v√†o b·∫£ng t·∫°m 
        with engine.begin() as conn:
            logger.info(f" A. Ghi d·ªØ li·ªáu v√†o b·∫£ng t·∫°m '{temp_table_name}' ")
            df.to_sql(
                temp_table_name,
                conn, 
                if_exists="replace",
                index=False,
                method='multi',
                chunksize=5000
            )
            logger.info(" GHI D·ªÆ LI·ªÜU V√ÄO B·∫¢NG T·∫†M TH√ÄNH C√îNG")
            
        # B∆∞·ªõc B: CHUY·ªÇN b·∫£ng t·∫°m th√†nh UNLOGGED
        with engine.begin() as conn:
            conn.execute(text(f'ALTER TABLE IF EXISTS "{temp_table_name}" SET UNLOGGED;'))
        logger.info(" ƒê√£ chuy·ªÉn b·∫£ng t·∫°m th√†nh UNLOGGED (tƒÉng t·ªëc ƒë·ªô ghi")
        
        # B∆∞·ªõc C: TH·ª∞C THI UPSERT
        with engine.begin() as conn: 
            logger.info(" TH·ª∞C THI L·ªÜNH UPSERT ")
            cols = ", ".join([f'"{c}"' for c in df.columns])
            upsert_query = f"""
            INSERT INTO "{table_name}" ({cols})
            SELECT {cols} FROM "{temp_table_name}"
            ON CONFLICT (location_id, datetime) DO NOTHING;
            """
            retry_execute(conn, upsert_query)
        logger.info(" ‚úÖ Upsert ho√†n t·∫•t th√†nh c√¥ng.")
        
    except Exception:
        logger.warning("\n L·ªói trong qu√° tr√¨nh Upsert (ƒë√£ rollback).")
        import traceback
        traceback.print_exc()
    
    finally:
        # --- B∆Ø·ªöC D: D·ªåN D·∫∏P ---
        logger.info(f"  -> C. D·ªçn d·∫πp b·∫£ng t·∫°m '{temp_table_name}'...")
        try:
            with engine.begin() as cleanup_conn:
                cleanup_conn.execute(text(f'DROP TABLE IF EXISTS "{temp_table_name}";'))
            logger.info("D·ªçn d·∫πp b·∫£ng t·∫°m th√†nh c√¥ng.")
        except Exception as cleanup_e:
            logger.warning(f"C·∫£nh b√°o: L·ªói khi d·ªçn d·∫πp b·∫£ng t·∫°m: {cleanup_e}")

    logger.info(f"üèÅ [Pipeline {batch_id}] Ho√†n t·∫•t upsert cho b·∫£ng '{table_name}'.\n")

# --- 4. H√†m ƒëi·ªÅu ph·ªëi ch√≠nh (Main orchestrator function) --- 
def run_realtime_etl():
    """
    H√†m ch√≠nh ƒë·ªÉ ƒëi·ªÅu ph·ªëi qu√° tr√¨nh ETL.
    """
    logger.info("==================================================")
    logger.info(f"B·∫ÆT ƒê·∫¶U ETL PIPELINE L√öC: {datetime.now()}")
    logger.info("==================================================")
    start_time = time.time()
    
    try: 
        # B∆∞·ªõc A: ƒê·ªçc metadata
        logger.info(f"\n [B∆∞·ªõc 1/3] ƒêang ƒë·ªçc metadata t·ª´ '{METADATA_FILE_PATH}'...")
        if not os.path.exists(METADATA_FILE_PATH):
            raise FileNotFoundError(f"L·ªói: Kh√¥ng t√¨m th·∫•y file metadata '{METADATA_FILE_PATH}'.")
        df_metadata = pd.read_csv(METADATA_FILE_PATH)
        logger.info(f" -> ƒê·ªçc th√†nh c√¥ng th√¥ng tin c·ªßa {len(df_metadata)} tr·∫°m.")
        
        # B∆∞·ªõc B: L·∫•y d·ªØ li·ªáu m·ªõi (Extract & Transform)
        logger.info("\n [B∆∞·ªõc 2/3] ƒêang l·∫•y d·ªØ li·ªáu g·∫ßn ƒë√¢y t·ª´ Open-Meteo...")
        recent_data_df = fetch_recent_data(df_metadata)
        
        # B∆∞·ªõc C: T·∫£i d·ªØ li·ªáu v√†o DB (Load)
        logger.info("\n [B∆∞·ªõc 3/3] ƒêang t·∫£i d·ªØ li·ªáu l√™n database...")
        if recent_data_df is not None and not recent_data_df.empty:
            db_engine = get_db_engine()
            upsert_data(db_engine, recent_data_df, DB_TABLE_NAME)
        else:
            logger.info(" -> Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ t·∫£i l√™n.")
    
    except Exception as e:
        logger.exception("ETL JOB TH·∫§T B·∫†I !!!")
        logger.warning(f"L·ªói: {e}")
    
    finally:
        end_time = time.time()
        logger.info("\n==================================================")
        logger.info(f"K·∫æT TH√öC ETL JOB. T·ªîNG TH·ªúI GIAN: {end_time - start_time:.2f} GI√ÇY.")
        logger.info(f" -> ƒê√£ ghi {len(recent_data_df)} b·∫£n ghi v√†o {DB_TABLE_NAME}.")
        logger.info("==================================================")
    
#--- 5. ƒêi·ªÉm b·∫Øt ƒë·∫ßu th·ª±c thi c·ªßa script ---
if __name__ == "__main__":
    run_realtime_etl()